* <2025-04-09 Wed> GE                                                :frågor:

1. Behöver det färdiga lexikonet verkligen passera genom transtool och
   därmmed vara beroende av de begränsningar som ordbasen där ger? Eller
   kan man kanske helt enkelt bara köra in det som finns i indata?
  Förmodligen inte ...

2. Vad händer om sv-material i indata inte (helt) överensstämmer med
   ordbasen? Finns ddet nåt sätt attt få med översättningarna ändå
   eller är sådant material bara att dumpa? Det kan gälla:
   - definition
   - sammansätttning
   - exempel
   - idiom
   - annat?
     
* <2025-04-17 Thu> GE
Lite kollar på täckningen för aktuell lexin sv-uppslag (unnder S) i
kelderashdaata:

#+begin_src bash
time grep '#01' s.txt | cut -f2- -d' ' | sed 's/\~//g' | raku api2.raku --status | grep '"Wordbase"' | sunis
#+end_src

: 1563       "Wordbase": "LSL4",
: 1439       "Wordbase": "LSL4"
: 
: real	2m12.952s
: user	0m57.943s
: sys 	0m4.713s

#+begin_src bash
time grep '#01' s.txt | cut -f2- -d' ' | sed 's/\~//g' | raku api2.raku --status > status_S.json
#+end_src

: real	2m16.439s
: user	0m58.635s
: sys 	0m4.699s

#+begin_src bash
grep '#01' s.txt | wc -l
#+end_src

:    3039

#+begin_src bash
grep '"Status"' status_S.json | sunis
#+end_src

: 1389       "Status": "found"
: 1369       "Status": "found",
:  147       "Status": "no unique matching",
:   52       "Status": "no unique matching"
:   37       "Status": "no matching"
:   31       "Status": "corrected",
:   14       "Status": "corrected"

<2025-04-26 Sat>
(Förhoppningsvis) samma körningssresultat med ny form för indata:

#+begin_src bash
   time cat ../kelderasch_filer/s.json | raku -MJSON::Fast -e '$*IN.slurp.&from-json.map: *.<#01>.subst("~", "|",:g).say' | raku api2.raku --status > status_swe-swe_S.json 
#+end_src

<2025-04-29 Tue>

Också för swe_fin (dvs. "Wordbase": "LSL3"):
#+begin_src bash
  time cat ../kelderasch_filer/s.json | raku -MJSON::Fast -e '$*IN.slurp.&from-json.map: *.<#01>.subst("~", "|",:g).say' | raku api2.raku --lang='swe>fin' --status > status_swe-fin_S.json 
#+end_src

Med statistik (för S):
#+begin_src bash
  cat  status_swe-fin_S.json | grep Status | raku -ne 'say .subst(/","\h*$/, "")' | sunis

  cat  status_swe-fin_S.json | grep ':' | raku -ne 'say .subst(/","\h*$/, "")' | sunis
#+end_src

: 2196       "Status": "found"
:  662       "Status": "corrected"
:  172       "Status": "no unique matching"
:    9       "Status": "no matching"

: 3030       "Wordbase": "LSL3"
: 2196       "Status": "found"
:  662       "Status": "corrected"
:  172       "Status": "no unique matching"
:    9       "Status": "no matching"

swe_swe (igen):
: 2994       "Wordbase": "LSL4"
: 2162       "Status": "found"
:  832       "Corrections": [
:  632       "Status": "corrected"
:  200       "Status": "no unique matching"
:   45       "Status": "no matching"

: 2162       "Status": "found"
:  632       "Status": "corrected"
:  200       "Status": "no unique matching"
:   45       "Status": "no matching"

* <2025-04-29 Tue>╭                                              :viggo_json:
Jag hade problem med att få körning mot =LSL3= dvs. =swe_fin= att gå
igenom. Efter ganska mycket letande hittade jag det i brister i den
json som produceras av lexins (json)-API.

Enda fallet (under boksatven S) som ställer till problem är:
#+begin_src
"Value": "omv\u00e4nt snedstreck",
"Type": "subst.",
"VariantID": "12445",
[ ... ]
"Meaning": "tecknet \", 
#+end_src

där alltså sekvensen '\"' är ett specialtecken för inbäddade
dubbelfnuttar och vad saknas för att få giltig json är escapning av
'\': 
#+begin_src 
"Meaning": "tecknet \\", 
#+end_src

Jag har löst det temporärt genom att skriva om just den strängen till
den escapade motsvarigheten, men ska kontakta Viggo i ärendet,
fler kan ju få problem, exempelvis =lexin.se= som vad jag förstår
använder json-apiet ....

* <2025-05-07 Wed>
Jag hittar allt fler fall där sökningar på det svenska uppslaget inte
ger det svar (beträffande ID och VariantID) som på nåt
tillfredställande sätt motsvarar informationen i
kelderashmaterialet. I vissa fall ID-värdena överenstämma men den
svenska diefinitionen eller nåt (svenskt) exempel skiljer sig åt. Även
då är det svårt att mappa samman dem ... Eller?

Jag borde sammanställa en liten hög med enxempel.

Jag kollade också lite på hur många oöversatta poster det finns i
materialet:

#+begin_src bash
cat ../kelderasch_filer/s.json | raku -MJSON::Fast -e 'for $*IN.slurp.&from-json { next if .<#31>; next if .<#02> ~~ /^"se "/; next if .<#54>; say .<#01> }' | wc -l
     156
#+end_src

#+begin_src bash
for f in  ../kelderasch_filer/*.json; do  cat $f | raku -MJSON::Fast -e 'for $*IN.slurp.&from-json { next if .<#31>; next if .<#02> ~~ /^"se "/; next if .<#54>; say .<#01> }'; done | wc -l
   2407
#+end_src

* <2025-05-14 Wed> GE                                                 :Viggo:

Viggo berättade att haan redan gjort en parser för "radformatet" och
producerat en xmlfil. Dock hittade han då ett antal fel i indata:

- en lista med "felstavningar"
- c:a 300 ord som bara saknas.

de saknade orden:
| löpnr | swe_fin                                                                                                |
|-------+--------------------------------------------------------------------------------------------------------|
| 16320 | <Word ID="14758" MatchingID="18880" Type="subst." Value="slipover" Variant="" VariantID="16320">       |
| 16321 | <saknas> <Word ID="14759" MatchingID="18881" Type="verb" Value="slipper" Variant="" VariantID="16321"> |
|   ... | <flera saknade ord>                                                                                    |
| 16686 | <saknas> <Word ID="15093" MatchingID="19301" Type="subst." Value="snus" Variant="" VariantID="16686">  |
| 16687 | <Word ID="15094" MatchingID="19302" Type="verb" Value="snusar" Variant="" VariantID="16687">           |

#+begin_src emacs-lisp
(- 16687 16320)
#+end_src

#+RESULTS:
: 367

Alltså 367 ord.

* <2025-05-15 Thu> GE
Jag har diffat lite impressionistiskt på Viggos fil
=lexinextra/dictionaries/swe_kal.txt= och våra/Bakis filer och tycker
mig se skilllnader åt båda håll. Alltså "våra" verkar bättre/nyare
ibland och ibland tvärtom. Vi behöver nog få (nån slags uppskattning
om) redigeringsdatum av både Baki och Viggo.

Om vi i slutändan ska göra nån slags egen diff på avsiktliga och
ovsiktliga skillnder måste vi sortera materialet ett sätt som
överensstämmer med Viggos sorteringsordning. Det borde ju vara en
trivial sak via unicodes =colllation order=, men det klarar varken
python (säger Magnus) eller raku (vet jag av egen bister
erfarenhet). Så nu började jag skissa på en rudimentär och pinsamt
förskoleliknade lösning:

#+begin_src raku
  my %swe is default(29) = <å 30 ä 31 ö 32>;
  sub swe ($a, $b) {
   if $a, $b ⊂ %swe.keys { # delmängd är förstås fel!
       %swe{$a.lc} cmp %swe{$b.lc} 
   } else { $a coll $b }
  } 


#+end_src

Nja,ovanstående funkar fast på fel grunder:
#+begin_src 
[16] > swe "Z","Ä"
Less
[17] > swe "B","b"
Same
[18] > swe "Ö","b"
More
[19] > swe "Ö","ä"
More
[20] > swe "å","ä"
Less
[21] > swe "Å","Ä"
Less
[22] > swe "Ä","Å"
More
[23] > %swe<A>
29
[24] > %swe<Ö>
29
[25] > %swe<ö>
32
[26] > <Ö A> (<) %swe.keys 
False
[27] > <ö a> (<) %swe.keys 
False
[28] > %swe.keys 
(ö ä å)
[29] > <ö a> (<) <a b c>
False
[30] > <ö ä> (<) <a b c>
False
[31] > <ö ä> (<) <a b c>.Set
False
[32] > <ö ä> (<) <a b c ö ä>
True
[33] > <ö ä> (<) <a b c ö>
False
[34] > <ö ä>.collate
(ä ö)
[35] > <ö ä å>.collate
(å ä ö)
[36] > <Å o O a ö ä m Ö n g å>.collate
(a å Å ä g m n o O ö Ö)
[37] > <ö ä å>.sort
(ä å ö)
[38] > 

#+end_src

#+begin_src raku :results output 
      sub swe ($a, $b) {
          my @in = ($a, $b)».lc; 
          my $swe = <å ä>.Set; 
          #say @in ~~ $swe; 
          

      #    given ($a, $b)».lc {
      #        when * ⊂ $swe and{ say "swe: "  }
      #        default { say "coll"  }
      #    }
    }

  swe "Ä", "Ä"; 
#+end_src 

#+RESULTS:
: False

#+begin_src raku :results output 
  subset Å   of Str where * ∈ <å Å>; 
  subset Ä   of Str where * ∈ <ä Ä>; 
  subset nÅÄ of Str where * ∉ <ä Ä å Å>; 

  multi swe (Å $a, Ä $b) { note 111; Less }
  multi swe (Ä $a, Å $b) { note 222; More }
  multi swe ($a, nÅÄ $b) { note 333; $a coll $b }
  multi swe (nÅÄ $a, $b) { note 444; $a coll $b }




       #my @in = ($a, $b)».lc; 
            #my $swe = <å ä>.Set; 
            #say @in ~~ $swe; 


        #    given ($a, $b)».lc {
        #        when * ⊂ $swe and{ say "swe: "  }
        #        default { say "coll"  }
        #    }
      #}

  say swe "ä", "å"; 
  say "ä" coll "å"; 

  say swe "ö", "o"; 
  say "ö" coll "o"; 
#+end_src 

#+RESULTS:
: More
: More
: More
: More

* <2025-05-28 Wed> GE                                                 :Viggo:
Lite mer konversation med Viggo:

* <2025-06-12 Thu>
småstartat diffning genmot Viggos swe_kal.txt:
#+begin_src bash
cat swe_kal.txt | head -100 | raku -e '$*IN.slurp.split(/^^[\h*\n]+/).map: { my %h = do for .split(/\h*\n\h*/, :skip-empty) -> $r {  my ($k, $v) = $r.split: /\h+/, 2; $k => $v }; dd %h}'
#+end_src


* <2025-06-25 Wed>
Viggo:

Här är den beskrivning av formatet som jag hittade. Förmodligen används inte alla koder i det aktuella lexikonet.
| #01 | uppslagsord, ev. följt av nummer (= xxx)                       |
| #02 | ordklassbeteckning                                             |
| #04 | ordförklaring                                                  |
| #05 | stilkommentar                                                  |
| #06 | sakupplysning                                                  |
| #07 | satsexempel; löpnummer i 47                                    |
| #08 | idiomexempel; löpnummer i 48                                   |
| #09 | referens till bild eller annat uttryck                         |
| #10 | syntaxmarkering, t ex "x & y"                                  |
| #11 | sammansättningsexempel; löpnummer i 51                         |
| #12 | ordböjning, hela ord eller inlett med "-" om 10 innehåller "~" |
| #13 | stavningsvariant                                               |
| #14 | uttal                                                          |
| #15 | grammatikkommentar                                             |
| #31 | övers. av #01                                                  |
| #32 | övers. av #02                                                  |
| #35 | övers. av #05                                                  |
| #36 | övers. av #06                                                  |
| #37 | övers. av #07                                                  |
| #38 | övers. av #08                                                  |
| #39 | övers. av #09                                                  |
| #41 | övers. av #11                                                  |
| #45 | övers. av #15                                                  |
| #54 | övers. av #04                                                  |
| #47 | löpnummer för exempel                                          |
| #48 | löpnummer för idiom                                            |
| #51 | löpnummer för sammansättning                                   |
| #98 | löpnummer för olika uppslagsord                                |
| #99 | löpnummer för uppslagsord (inkl varianter)                     |
 

Fortsatt och committat viggoprepareringen i
=viggo-swe_kal2json.raku=. Men sammtidigt hittat strukturproblem: en
del enklare är jag ändrat för hand i =swe_kal-red.txt=. Andra är
svårare: multipla källfält med motsv översättningsfält. Problemen är
gemensamma för Baki- och Viggomaterialen:

#+begin_src json
    "#01": "övning",
    "#31": "zumavel",
    "#02": "subst.",
    "#32": "substantivo",
    "#04": "det att öva, träning",
    "#05": "även om den uppgift som övas",
    "#35": "",
    "#07": "övning ger färdighet",
    "#37": "",
    "#47": "12459",
    "#11": "övnings~uppgift  en",
    "#41": "",
    "#51": "8136",
    "#12": "övningen övningar",
    "#14": "2Ö:vni@",
    "#98": "19706",
    "#99": "21876",
    "#54": "zumavel",
    "#97": "mini"
  },
#+end_src

#+begin_example
#01 övning
#31 zumavel
#02 subst.
#32 substantivo
#04 det att öva, träning
#05 även om den uppgift som övas
#35 
#07 övning ger färdighet
#37 
#47 12459
#11 skrivövning
#41 
#51 8134
#11 militärövning
#41 
#51 8135
#11 övnings~uppgift  en
#41 
#51 8136
#12 övningen övningar
#14 2Ö:vni@
#98 19706
#99 21876
#54 zumavel
#97 mini
#+end_example

#+begin_quote
JL> Så får Baki kika på dom
 
GE> Jo, fast hur väljer vi ut data så att vi säkra på vad som ska
användas (dvs. diffningen mot Viggo)

Och sen hur ska man kunna ge Baki dde verkligt relevanta skillnaderna
när diffningen inte är klockren.

Kanske bäst att helt skita i steget med jämförelse mot Viggo till
priset av (förmodligen) sämre kvalitet?

JL> Hmm 
Du menar att man kör med "Viggo versionen"?

GE> Nä, i såna fall Bakiversionen

JS> Det kanske är bättre från ett "peace-of-mind" perspektiv
tillsvidare Jag misstänker ju att om vi slänger massa diffar i bakis
knä får vi vänta ett bra tag

GE> Jo, kanske. Men poängen med diffning skulle ju vara att välja ut
de viktiga fallen och och skona Baki från trivialdiffar av typen
”komma el semikolon”, ”tomt el. fyllt fält”, etc

Och att få Baki att hitta saker han inte skulle ha hittat annars …

Men i såna fall skulle mitt/vårt jobb bestå av att hitta rena
strukturella fel (jag har några från Viggofillen) och sen leverera
innehållet som det ser ut nu.
#+end_quote

* <2025-08-13 Wed>
Igång igen efter sommmaren. Det verkar efter funderingar på
ovanstående under sommaren som att det är exakt det skisserade som
ska göras. 

** DONE ny textkonvertering (med soffice)
Detta för att alla tomrader verkar ha försvunnit i förra (pandoc-?)
konverteringen.
#+begin_src bash
  cd kelderasch_filer
  soffice --headless --convert-to txt --outdir text *.docx 
#+end_src

En ev. nackdel med sofficekonvertering är att denna lägger till en
BOM-markör i början av varje fil, nåt som tydligen windows gillar. Men
eftersomm Viggos resulltatfiler från förra omgången också innehåller
såna så antar jag att det inte kommer ge honomm några problem.

En liten koll:
#+begin_src bash
file kelderasch_filer/text/*.txt
#+end_src

#+RESULTS:
: a.txt: Unicode text, UTF-8 (with BOM) text
: b.txt: Unicode text, UTF-8 (with BOM) text
: c.txt: Unicode text, UTF-8 (with BOM) text
: d.txt: Unicode text, UTF-8 (with BOM) text, with very long lines (301)
: e.txt: Unicode text, UTF-8 (with BOM) text
: f.txt: Unicode text, UTF-8 (with BOM) text
: g.txt: Unicode text, UTF-8 (with BOM) text
: h.txt: Unicode text, UTF-8 (with BOM) text
: i.txt: Unicode text, UTF-8 (with BOM) text
: j.txt: Unicode text, UTF-8 (with BOM) text
: k.txt: Unicode text, UTF-8 (with BOM) text
: l.txt: Unicode text, UTF-8 (with BOM) text
: m.txt: Unicode text, UTF-8 (with BOM) text
: n.txt: Unicode text, UTF-8 (with BOM) text
: o.txt: Unicode text, UTF-8 (with BOM) text
: p.txt: Unicode text, UTF-8 (with BOM) text
: q.txt: Unicode text, UTF-8 (with BOM) text
: r.txt: Unicode text, UTF-8 (with BOM) text
: s.txt: Unicode text, UTF-8 (with BOM) text
: t.txt: Unicode text, UTF-8 (with BOM) text
: u.txt: Unicode text, UTF-8 (with BOM) text
: v.txt: Unicode text, UTF-8 (with BOM) text
: w.txt: Unicode text, UTF-8 (with BOM) text
: x.txt: Unicode text, UTF-8 (with BOM) text
: y.txt: Unicode text, UTF-8 (with BOM) text
: z.txt: Unicode text, UTF-8 (with BOM) text
: ä.txt: Unicode text, UTF-8 (with BOM) text
: å.txt: Unicode text, UTF-8 (with BOM) text
: ö.txt: Unicode text, UTF-8 (with BOM) text

De långa raderna i d är inget syntaaxproblem utan bara väldigt
babbliga förklaringar.

** <2025-08-24 Sun>
Innan jag började prata med Viggo igen ville jag ha lite bättre
koll på skillnaderna melllan de olika materialen igen:

#+begin_src bash
  for f in a b c d e f g h i j k l m n o p q r s t u v w x y z ä å ö; 
     do 
        echo $f; 
        cat ../../lexinextra-dictionaries/dictionaries/swe_kal.txt | raku -Msigpipe -e '
                    my $fil = @*ARGS[0]; 
                    say join "\n\n", $*IN.slurp.
                                         split(/\n\s* <?before "#01 ">/, :skip-empty).
                                         grep: /:i ^ "#01 $fil"/' $f  > swe_kal/$f.txt ; 
     done
#+end_src

#+begin_src bash 
  wc -l swe_kal/*.txt
  ##
  wc -l ../kelderasch_filer/text/*.txt 
#+end_src 

#+begin_src bash
 for f in a b c d e f g h i j k l m n o p q r s t u v w x y z ä å ö; do echo $f; diff -U 0  ../kelderasch_filer/text/$f.txt swe_kal/$f.txt > diff/$f.diff; done
#+end_src

#+begin_src bash
  cd diff
  for f in a b c d e f g h i j k l m n o p q r s t u v w x y z ä å ö; do echo $f; grep '^@@' $f.diff | wc -l; echo %%; done | raku -e '.say for $*IN.slurp.split("%%")».split(/\s+/, :skip-empty)'
#+end_src


Eter lite emacsslöjd:
| fil   | diffar | rader (översättare) | rader (Viggo) |
|-------+--------+---------------------+---------------|
| a.txt |    330 |               16818 |         16731 |
| b.txt |    985 |               24073 |         24071 |
| c.txt |     82 |                2302 |          2300 |
| d.txt |    113 |               12479 |         12479 |
| e.txt |    128 |                6527 |          6526 |
| f.txt |    550 |               26603 |         26603 |
| g.txt |    183 |               10739 |         10740 |
| h.txt |    305 |               13252 |         13228 |
| i.txt |    231 |                7952 |          7951 |
| j.txt |    123 |                2946 |          2950 |
| k.txt |    190 |               22359 |         22357 |
| l.txt |    145 |               12640 |         12647 |
| m.txt |    264 |               15762 |         15763 |
| n.txt |    121 |                7067 |          7070 |
| o.txt |    118 |               10169 |         10169 |
| p.txt |    144 |               17318 |         17300 |
| q.txt |      2 |                  12 |            11 |
| r.txt |    172 |               16609 |         16611 |
| s.txt |   1712 |               45232 |         45154 |
| t.txt |    267 |               18862 |         18838 |
| u.txt |    508 |               10626 |         10627 |
| v.txt |    205 |               14491 |         14454 |
| w.txt |     19 |                 215 |           215 |
| x.txt |      3 |                  25 |            24 |
| y.txt |     16 |                 775 |           773 |
| z.txt |      6 |                 159 |           158 |
| ä.txt |     70 |                1942 |          1943 |
| å.txt |    171 |                2605 |          2605 |
| ö.txt |     19 |                4209 |          4196 |
|-------+--------+---------------------+---------------|
|       |        |              324768 |        324494 |


Kollar lite detaljer:
- Så gott som alltid skillnader i översättningar. Mestaadels
  grafematiska skillnader.


** <2025-08-25 Mon>

*** DONE De 524 transtoolorden
Här vet jag inte alls hur jag ska göra. Det är väldigt många
parametrar!

Det mest konkreta hittill är Jacobs förslag att filtrera fram de
översatta svenska orden och slå upp dem i typ somaliska för att se att
de finns i ordbas 3.

<2025-09-08 Mon>
Dessa ord bestämde vi i förra veckan att vi skullle ignorera.

*** DONE Checkar av de ofullkommligheter som Viggo upptäckte för några år sedan:

**** DONE Alla ord mellan slipover och snusar saknas.
<2025-09-06 Sat>

#+begin_src bash
grep '<Word '  ../../lexinextra-dictionaries/dictionaries/combined/swe_fin.xml | \
perl -nE '/Value="(.*?)"/ and say $1' | \
raku -e '$*IN.slurp.match(/"slipover" \s*? <(.+?)> "snusar"/).Str.say' | \
egrep -v '^\s*$' > slipover-snusar_swefin
#+end_src

Sen lite slöjande av resultatfilen till:
#+begin_src 
#01 sned
#01 snedsprång
#01 snedsteg
#01 snedstreck
#01 snedtänder
#01 sned~vrider
...
#+end_src

Och:
#+begin_src bash
cd ../kelderasch_filer/text/
fgrep -f../../lexin/slipover-snusar_swefin * 
#+end_src

som ger:
#+begin_src 
s.txt:#01 snusar
s.txt:#01 snus~förnuftig
s.txt:#01 snusk
s.txt:#01 snusk~hummer
s.txt:#01 snuskig
s.txt:#01 snus~torr
#+end_src

Alltså inga ord mellan /slipover/ och /snusar/ gömda på fel ställle.

**** DONE Ord som inte finns kvar i svenska ordbasen:
Ta bort!

***** DONE SAF
***** DONE serbo~kroatisk
***** DONE serbo~kroatiska
***** DONE SIV
***** DONE SJ
***** DONE sjuk~bidrag
***** DONE skydds~konsulent
***** DONE SOS
***** DONE SR-koncernen
***** DONE Statens invandrarverk (SIV)
***** DONE Statens vägverk
***** DONE Svenska Arbetsgivareföreningen

**** DONE Felstavningar:

***** DONE #01 sensitivet
-->
#01 sensitivitet

***** DONE #01 själ lös
-->
#01 själlös

***** DONE #01 spekulrar
-->
#01 spekulerar

***** DONE #11 stipendie~u elning -en
-->
#11 stipendie~utdelning -en

***** DONE #07 ett tvåårigt straff u ömdes
-->
#07 ett tvåårigt straff utdömdes

***** DONE #07 verksamheten sponsras av Volvo
#37 o inkeripe sponsoril o Volvo
-->
#07 verksamheten sponsras av xxxx
#37 o inkeripe sponsoril o xxxx

**** DONE Fel fält: 
#38 är översättning av idiom, och det får inte förekomma om det inte finns ett svenskt idiom före. Här kommer #38 efter #04 som är ordförklaring:
#04 spott och spe ("hån")
#38 čungar ai prasape,





























